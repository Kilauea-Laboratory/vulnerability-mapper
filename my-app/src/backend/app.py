import os
import re
import string
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
from nltk.stem import PorterStemmer
import torch
import transformers
from transformers import BertTokenizer
from torch.utils.data import Dataset, DataLoader
from flask import Flask, request, jsonify

MAX_LEN = 100
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

app = Flask(__name__)

class SingleTextInferenceDataset(Dataset):
    def __init__(self, text, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.text = self.preprocess_text(text)
        self.max_len = max_len

    def preprocess_text(self, text):
        text = clean_text(text)
        text = remove_stopwords(text)
        text = stem_words(text)
        return text

    def __len__(self):
        return 1

    def __getitem__(self, _):
        inputs = self.tokenizer.encode_plus(
            self.text,
            None,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            return_token_type_ids=True
        )
        ids = inputs['input_ids']
        mask = inputs['attention_mask']
        token_type_ids = inputs["token_type_ids"]

        return {
            'ids': torch.tensor(ids, dtype=torch.long),
            'mask': torch.tensor(mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
        }

def clean_text(text):
    text = text.lower()
    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)
    text = re.sub('[_{}]', ' ', text)
    text = re.sub('\d+', '', text)
    text = re.sub(r'\b\w{1,2}\b', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def remove_stopwords(text):
    stopwords_txt = 'stopwords.txt'
    with open(stopwords_txt, 'r') as file:
        custom_stopwords = set(word.strip() for word in file)

    STOPWORDS = set(stopwords.words('english'))
    combined_stopwords = STOPWORDS.union(custom_stopwords)

    return " ".join([word for word in str(text).split() if word not in combined_stopwords])

def stem_words(text):
    stemmer = PorterStemmer()
    return " ".join([stemmer.stem(word) for word in text.split()])

class BERTClass(torch.nn.Module):
    def __init__(self, num_classes):
        super(BERTClass, self).__init__()
        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')
        self.l2 = torch.nn.Dropout(0.3)
        self.l3 = torch.nn.Linear(768, num_classes)

    def forward(self, ids, mask, token_type_ids):
        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)
        output_2 = self.l2(output_1)
        output = self.l3(output_2)
        return output

num_classes = 5
model = BERTClass(num_classes)
model.to(device)

def predict_single_text(text, tokenizer, model, max_len):
    model.eval()

    text = clean_text(text)
    text = remove_stopwords(text)
    text = stem_words(text)

    single_text_dataset = SingleTextInferenceDataset(text, tokenizer, max_len)
    single_text_data_loader = DataLoader(single_text_dataset, batch_size=1, shuffle=False)

    with torch.no_grad():
        for batch in single_text_data_loader:
            ids = batch['ids'].to(device, dtype=torch.long)
            mask = batch['mask'].to(device, dtype=torch.long)
            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)

            outputs = model(ids, mask, token_type_ids)

            predicted_labels = torch.sigmoid(outputs).cpu().detach().numpy().tolist()

    return predicted_labels

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json()
    text_to_predict = data['text']

    predicted_labels = predict_single_text(text_to_predict, tokenizer, model, MAX_LEN)

    return jsonify({'prediction': predicted_labels})

if __name__ == '__main__':
    script_directory = os.path.dirname(os.path.abspath(__file__))

    model_path = os.path.join(script_directory, "models", "mitigation_model.pth")

    model.load_state_dict(torch.load(model_path, map_location=device))
    model.eval()

    app.run(debug=True)
